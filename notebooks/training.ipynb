{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtxKg16t3hEv",
        "outputId": "e1c8c1cb-2612-4018-9400-9c717aad1a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 113 kB 84.1 MB/s \n",
            "\u001b[?25h  Building wheel for importRosbag (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 92 kB 597 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!pip install tonic --quiet\n",
        "!pip install snntorch --quiet\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import ast\n",
        "import csv\n",
        "import torch\n",
        "import tonic\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import snntorch as snn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import tonic.transforms as transforms\n",
        "from snntorch import utils\n",
        "from snntorch import surrogate \n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "SRC = \"/content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/\"\n",
        "\n",
        "# List of .csv files with event data\n",
        "csv_files = [SRC + 'GT_HW1_1_DROPPED.csv', SRC + 'GT_HW1_3_DROPPED.csv', SRC + 'GT_HW1_4_DROPPED.csv', SRC + 'GT_HW2_1_DROPPED.csv', \n",
        "             SRC + 'GT_HW2_2_DROPPED.csv', SRC + 'GT_HW2_3_DROPPED.csv', SRC + 'GT_HW2_4_DROPPED.csv', SRC + 'GT_HW2_5_DROPPED.csv', \n",
        "             SRC + 'GT_HW2_6_DROPPED.csv', SRC + 'GT_LAMBDA_1_DROPPED.csv', SRC + 'GT_LAMBDA_2_DROPPED.csv', SRC + 'GT_LAMBDA_3_DROPPED.csv', \n",
        "             SRC + 'GT_LAMBDA_4_DROPPED.csv', SRC + 'GT_LAMBDA_5_DROPPED.csv', SRC + 'GT_LAMBDA_6_DROPPED.csv', SRC + 'GT_LAMBDA_7_DROPPED.csv', \n",
        "             SRC + 'GT_LAMBDA_8_DROPPED.csv', SRC + 'GT_BOX1_DROPPED.csv', SRC + 'GT_BOX2_DROPPED.csv', SRC + 'GT_FLOOR_DROPPED.csv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy3tak9HBEU_",
        "outputId": "b5ec0ca4-4b25-4ab5-d032-ecd44f36396e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr  1 18:41:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAt-RYq_8gNq",
        "outputId": "de5a3745-ceba-4e10-c2a6-d7acf2c817f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW1_1_DROPPED.csv\n",
            "Number of data points:  899\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW1_3_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW1_4_DROPPED.csv\n",
            "Number of data points:  238\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW2_1_DROPPED.csv\n",
            "Number of data points:  899\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW2_2_DROPPED.csv\n",
            "Number of data points:  897\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW2_3_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW2_4_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW2_5_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_HW2_6_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_1_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_2_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_3_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_4_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_5_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_6_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_7_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_LAMBDA_8_DROPPED.csv\n",
            "Number of data points:  898\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_BOX1_DROPPED.csv\n",
            "Number of data points:  2137\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_BOX2_DROPPED.csv\n",
            "Number of data points:  6130\n",
            "Read:  /content/drive/Shareddrives/Shilpa_Thesis/processed_data/dropped/GT_FLOOR_DROPPED.csv\n",
            "Number of data points:  2058\n",
            "Length of all data points:  24932\n"
          ]
        }
      ],
      "source": [
        "df_list = []\n",
        "for file in csv_files:\n",
        "  # Drop unecessary columns when reading in the dataframes\n",
        "  df = pd.read_csv(file, index_col = 0).reset_index(drop = True)\n",
        "  print(\"Read: \", file)\n",
        "  print(\"Number of data points: \", len(df))\n",
        "  del df['index']\n",
        "  df_list.append(df)\n",
        "concat_df = pd.concat(df_list).reset_index(drop = True)\n",
        "print(\"Length of all data points: \", len(concat_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XmD4wge9E8y",
        "outputId": "6f722ebc-5a5e-471d-95f7-ad8cedc669fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size:  14959\n",
            "Validation dataset size:  6233\n",
            "Test dataset size:  3740\n"
          ]
        }
      ],
      "source": [
        "# Shuffle the dataframe rows and reset the index before training/validation/test split\n",
        "shuffled_df = concat_df.sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "# Split the concatenated dataframe into training and test subsets\n",
        "train_df, validation_df = train_test_split(concat_df, test_size = 0.25)\n",
        "train_df, test_df = train_test_split(train_df, test_size = 0.20)\n",
        "print(\"Training dataset size: \", len(train_df))\n",
        "print(\"Validation dataset size: \", len(validation_df))\n",
        "print(\"Test dataset size: \", len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p6xwY28-4Nzy"
      },
      "outputs": [],
      "source": [
        "class SyntheticRecording(tonic.Dataset):\n",
        "  \"\"\"\n",
        "      Synthetic event camera recordings dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, df):\n",
        "    super(SyntheticRecording, self).__init__()\n",
        "    self.df = df.reset_index(drop = True) # Address index out of order issue\n",
        "    self.events = self.df['Events'] # Select only last column of dataframe\n",
        "    self.target = self.df[['Vel_x', 'Vel_y', 'Vel_z']] # Select every column except last column of dataframe\n",
        "    self.sensor_size = (1920, 1080, 2)\n",
        "    # Denoise removes isolated, one-off events\n",
        "    self.frame_transform = transforms.Compose([transforms.Denoise(filter_time = 1000000),\n",
        "                                               transforms.ToFrame(sensor_size = (1920, 1080, 2), n_event_bins = 5)]) \n",
        "    \n",
        "  \"\"\"\n",
        "      Retrieve the index i to get the ith sample from the dataset. Apply the appropriate transformations.\n",
        "  \"\"\"\n",
        "  def __getitem__(self, index):\n",
        "    list_ = ast.literal_eval(self.events[index]) # Convert string literal to list\n",
        "    t, x, y, p = [], [], [], []\n",
        "    for e in list_:\n",
        "      t.append(e[0] * 1e6) # Convert to microseconds\n",
        "      x.append(e[1])\n",
        "      y.append(e[2])\n",
        "      p.append(e[3])\n",
        "    structured_events = tonic.io.make_structured_array(x, y, t, p) # Ordering is xytp now\n",
        "    transformed_frames = self.frame_transform(structured_events)\n",
        "    vel_xyz = []\n",
        "    vel_x = self.target.loc[index][0]\n",
        "    vel_xyz.append(vel_x)\n",
        "    vel_y = self.target.loc[index][1]\n",
        "    vel_xyz.append(vel_y)\n",
        "    vel_z = self.target.loc[index][2]\n",
        "    vel_xyz.append(vel_z)\n",
        "\n",
        "    frames_tensor = torch.tensor(transformed_frames)\n",
        "    vel_tensor = torch.tensor(vel_xyz, dtype = torch.float32)\n",
        "    return frames_tensor, vel_tensor\n",
        "\n",
        "  \"\"\"\n",
        "      Returns the size of the dataset.\n",
        "  \"\"\"\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "class PadMultiOutputTensor:\n",
        "  def __init__(self, batch_first: bool = False):\n",
        "    self.batch_first = batch_first\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    samples_output = []\n",
        "    targets_output = []\n",
        "\n",
        "    max_length = max([sample.shape[0] for sample, target in batch])\n",
        "    for sample, target in batch:\n",
        "      if isinstance(sample, torch.Tensor):\n",
        "        sample = torch.tensor(sample)\n",
        "        samples_output.append(\n",
        "            torch.cat((sample,\n",
        "                       torch.zeros(max_length - sample.shape[0], *sample.shape[1:]),)))\n",
        "        targets_output.append(target)\n",
        "        \n",
        "    return (torch.stack(samples_output, 0 if self.batch_first else 1),\n",
        "            torch.stack(targets_output),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NWgKCbuQKg1i"
      },
      "outputs": [],
      "source": [
        "# Create PyTorch dataset objects\n",
        "train_dataset = SyntheticRecording(train_df) \n",
        "val_dataset = SyntheticRecording(validation_df)\n",
        "test_dataset = SyntheticRecording(test_df)\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size = 1,\n",
        "                          collate_fn = PadMultiOutputTensor())\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1,\n",
        "                        collate_fn = PadMultiOutputTensor())\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1,\n",
        "                         collate_fn = PadMultiOutputTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxNHz3asLecu"
      },
      "outputs": [],
      "source": [
        "class MultiOutputSNN(nn.Module):\n",
        "  def __init__(self, beta, spike_grad):\n",
        "    super(MultiOutputSNN, self).__init__()\n",
        "    self.beta = beta\n",
        "    self.spike_grad = spike_grad\n",
        "\n",
        "    # Initialize the layers\n",
        "    self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 8, kernel_size = 5)\n",
        "    self.lif1 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.lif2 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.lif3 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5)\n",
        "    self.lif4 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.lif5 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.lif6 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.flat = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(in_features = 16 * 267 * 477, out_features = 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mem1 = self.lif1.init_leaky()\n",
        "    mem2 = self.lif2.init_leaky()\n",
        "    mem3 = self.lif3.init_leaky()\n",
        "    mem4 = self.lif4.init_leaky()\n",
        "    mem5 = self.lif5.init_leaky()\n",
        "    mem6 = self.lif6.init_leaky()\n",
        "\n",
        "    vel_xyz = None\n",
        "\n",
        "    for step in range(x.size(0)):\n",
        "      res = F.max_pool2d(self.conv1(x[step]), (2, 2))\n",
        "      spk1, mem1 = self.lif1(res, mem1)\n",
        "      spk2, mem2 = self.lif2(spk1, mem2)\n",
        "      spk3, mem3 = self.lif3(spk2, mem3)\n",
        "      res2 = F.max_pool2d(self.conv2(spk3), (2, 2))\n",
        "      spk4, mem4 = self.lif4(res2, mem4)\n",
        "      spk5, mem5 = self.lif5(spk4, mem5)\n",
        "      spk6, mem6 = self.lif4(spk5, mem6)\n",
        "      flat = self.flat(spk6)\n",
        "      vel_xyz = self.fc1(flat)\n",
        "      \n",
        "    return vel_xyz\n",
        "\n",
        "class MultiOutputSCNN816(nn.Module):\n",
        "  def __init__(self, alpha, beta, spike_grad):\n",
        "    super(MultiOutputSCNN816, self).__init__()\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.spike_grad = spike_grad\n",
        "\n",
        "    # Initialize the layers\n",
        "    self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 8, kernel_size = 5)\n",
        "    self.lif1 = snn.Synaptic(alpha = self.alpha, beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5)\n",
        "    self.lif2 = snn.Synaptic(alpha = self.alpha, beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.flat = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(in_features = 16 * 267 * 477, out_features = 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    syn1, mem1 = self.lif1.init_synaptic()\n",
        "    syn2, mem2 = self.lif2.init_synaptic()\n",
        "\n",
        "    vel_xyz = None\n",
        "\n",
        "    for step in range(x.size(0)):\n",
        "      res = F.max_pool2d(self.conv1(x[step]), (2, 2))\n",
        "      spk1, syn1, mem1 = self.lif1(res, syn1, mem1)\n",
        "      res2 = F.max_pool2d(self.conv2(spk1), (2, 2))\n",
        "      spk2, syn2, mem2 = self.lif2(res2, syn2, mem2)\n",
        "      flat = self.flat(spk2)\n",
        "      vel_xyz = self.fc1(flat)\n",
        "      \n",
        "    return vel_xyz\n",
        "\n",
        "class MultiOutputSCNN1232(nn.Module):\n",
        "  def __init__(self, beta, spike_grad):\n",
        "    super(MultiOutputSCNN1232, self).__init__()\n",
        "    self.beta = beta\n",
        "    self.spike_grad = spike_grad\n",
        "\n",
        "    # Initialize the layers\n",
        "    self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 12, kernel_size = 5)\n",
        "    self.lif1 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 12, out_channels = 32, kernel_size = 5)\n",
        "    self.lif2 = snn.Leaky(beta = self.beta, spike_grad = self.spike_grad)\n",
        "    self.flat = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(in_features = 32 * 267 * 477, out_features = 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mem1 = self.lif1.init_leaky()\n",
        "    mem2 = self.lif2.init_leaky()\n",
        "\n",
        "    vel_xyz = None\n",
        "\n",
        "    for step in range(x.size(0)):\n",
        "      res = F.max_pool2d(self.conv1(x[step]), (2, 2))\n",
        "      spk1, mem1 = self.lif1(res, mem1)\n",
        "      res2 = F.max_pool2d(self.conv2(spk1), (2, 2))\n",
        "      spk2, mem2 = self.lif2(res2, mem2)\n",
        "      flat = self.flat(spk2)\n",
        "      vel_xyz = self.fc1(flat)\n",
        "      \n",
        "    return vel_xyz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zjheWiCaCbfj"
      },
      "outputs": [],
      "source": [
        "# Neuron and simulation parameters\n",
        "spike_grad = surrogate.fast_sigmoid(slope = 75)\n",
        "beta = 0.5\n",
        "alpha = 0.6\n",
        "torch.cuda.empty_cache()\n",
        "net = MultiOutputSCNN816(beta, alpha, spike_grad).cuda()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-8)\n",
        "loss_fn = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuntJBHL6xhe",
        "outputId": "2a5b19da-c014-4e63-8fd5-742a62d77d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Iteration 0: Training RMSE: 0.3360332250595093, Training MAE: 0.2609909176826477\n",
            "Training Iteration 10: Training RMSE: 0.10863696038722992, Training MAE: 0.08863562345504761\n",
            "Training Iteration 20: Training RMSE: 0.2526199519634247, Training MAE: 0.2097843736410141\n",
            "Training Iteration 30: Training RMSE: 0.07765820622444153, Training MAE: 0.06221061944961548\n",
            "Training Iteration 40: Training RMSE: 0.003842197824269533, Training MAE: 0.0036070928908884525\n",
            "Training Iteration 50: Training RMSE: 0.005278261844068766, Training MAE: 0.004791509360074997\n",
            "Training Iteration 60: Training RMSE: 1.6755582094192505, Training MAE: 1.504555106163025\n",
            "Training Iteration 70: Training RMSE: 0.1525135487318039, Training MAE: 0.10391738265752792\n",
            "Training Iteration 80: Training RMSE: 0.04010287672281265, Training MAE: 0.03580465167760849\n",
            "Training Iteration 90: Training RMSE: 0.9316888451576233, Training MAE: 0.8725128173828125\n",
            "Training Iteration 100: Training RMSE: 0.36847591400146484, Training MAE: 0.23679165542125702\n",
            "Training Iteration 110: Training RMSE: 0.18926933407783508, Training MAE: 0.15751339495182037\n",
            "Training Iteration 120: Training RMSE: 0.2312924712896347, Training MAE: 0.1727270781993866\n",
            "Training Iteration 130: Training RMSE: 0.895751416683197, Training MAE: 0.6657170057296753\n",
            "Training Iteration 140: Training RMSE: 0.1598566621541977, Training MAE: 0.15324300527572632\n",
            "Training Iteration 150: Training RMSE: 0.19778293371200562, Training MAE: 0.15780667960643768\n",
            "Training Iteration 160: Training RMSE: 0.6527695059776306, Training MAE: 0.5625169277191162\n",
            "Training Iteration 170: Training RMSE: 0.2912302613258362, Training MAE: 0.2282450795173645\n",
            "Training Iteration 180: Training RMSE: 0.25045663118362427, Training MAE: 0.22527019679546356\n",
            "Training Iteration 190: Training RMSE: 0.08466637134552002, Training MAE: 0.06969921290874481\n",
            "Training Iteration 200: Training RMSE: 0.2579141855239868, Training MAE: 0.21258673071861267\n",
            "Training Iteration 210: Training RMSE: 0.13050848245620728, Training MAE: 0.12904170155525208\n",
            "Training Iteration 220: Training RMSE: 0.17240995168685913, Training MAE: 0.1431695520877838\n",
            "Training Iteration 230: Training RMSE: 0.3723895847797394, Training MAE: 0.25152844190597534\n",
            "Training Iteration 240: Training RMSE: 0.1999095231294632, Training MAE: 0.1590214967727661\n",
            "Training Iteration 250: Training RMSE: 0.8406744003295898, Training MAE: 0.7559447288513184\n",
            "Training Iteration 260: Training RMSE: 0.12578031420707703, Training MAE: 0.11215674132108688\n",
            "Training Iteration 270: Training RMSE: 0.15712334215641022, Training MAE: 0.11954259872436523\n",
            "Training Iteration 280: Training RMSE: 0.15081137418746948, Training MAE: 0.13554731011390686\n",
            "Training Iteration 290: Training RMSE: 0.21595126390457153, Training MAE: 0.1747879683971405\n",
            "Training Iteration 300: Training RMSE: 0.38745400309562683, Training MAE: 0.27352920174598694\n",
            "Training Iteration 310: Training RMSE: 1.0238269567489624, Training MAE: 0.8362666964530945\n",
            "Training Iteration 320: Training RMSE: 0.15602673590183258, Training MAE: 0.1339189112186432\n",
            "Training Iteration 330: Training RMSE: 0.2781074643135071, Training MAE: 0.21948377788066864\n",
            "Training Iteration 340: Training RMSE: 0.43291082978248596, Training MAE: 0.38802751898765564\n",
            "Training Iteration 350: Training RMSE: 0.2646476626396179, Training MAE: 0.21416711807250977\n",
            "Training Iteration 360: Training RMSE: 0.2537446916103363, Training MAE: 0.2031346559524536\n",
            "Training Iteration 370: Training RMSE: 0.10510090738534927, Training MAE: 0.09903006255626678\n",
            "Training Iteration 380: Training RMSE: 0.23384025692939758, Training MAE: 0.19967207312583923\n",
            "Training Iteration 390: Training RMSE: 0.32176533341407776, Training MAE: 0.2194894552230835\n",
            "Training Iteration 400: Training RMSE: 0.2718241810798645, Training MAE: 0.26974400877952576\n",
            "Training Iteration 410: Training RMSE: 0.23448379337787628, Training MAE: 0.17241701483726501\n",
            "Training Iteration 420: Training RMSE: 0.17367108166217804, Training MAE: 0.1391632854938507\n",
            "Training Iteration 430: Training RMSE: 0.5033292770385742, Training MAE: 0.41319212317466736\n",
            "Training Iteration 440: Training RMSE: 0.6784860491752625, Training MAE: 0.5337241888046265\n",
            "Training Iteration 450: Training RMSE: 0.08532246202230453, Training MAE: 0.05398954078555107\n",
            "Training Iteration 460: Training RMSE: 0.01333472691476345, Training MAE: 0.011373932473361492\n",
            "Training Iteration 470: Training RMSE: 0.4488287568092346, Training MAE: 0.34095296263694763\n",
            "Training Iteration 480: Training RMSE: 1.5947784185409546, Training MAE: 1.547187089920044\n",
            "Training Iteration 490: Training RMSE: 0.3433898389339447, Training MAE: 0.31940531730651855\n",
            "Training Iteration 500: Training RMSE: 0.2681400179862976, Training MAE: 0.2401970624923706\n",
            "Training Iteration 510: Training RMSE: 0.47247380018234253, Training MAE: 0.3719722032546997\n",
            "Training Iteration 520: Training RMSE: 0.030730681493878365, Training MAE: 0.030645519495010376\n",
            "Training Iteration 530: Training RMSE: 0.8264178037643433, Training MAE: 0.6934666633605957\n",
            "Training Iteration 540: Training RMSE: 0.21723182499408722, Training MAE: 0.21074271202087402\n",
            "Training Iteration 550: Training RMSE: 0.3348219692707062, Training MAE: 0.26750001311302185\n",
            "Training Iteration 560: Training RMSE: 0.22447092831134796, Training MAE: 0.19093826413154602\n",
            "Training Iteration 570: Training RMSE: 0.24539557099342346, Training MAE: 0.2166351079940796\n",
            "Training Iteration 580: Training RMSE: 0.24591998755931854, Training MAE: 0.1804112195968628\n",
            "Training Iteration 590: Training RMSE: 1.0083155632019043, Training MAE: 0.8260087370872498\n",
            "Training Iteration 600: Training RMSE: 1.2248313426971436, Training MAE: 1.0336406230926514\n",
            "Training Iteration 610: Training RMSE: 0.07136882841587067, Training MAE: 0.05994487926363945\n",
            "Training Iteration 620: Training RMSE: 0.02788419835269451, Training MAE: 0.022576358169317245\n",
            "Training Iteration 630: Training RMSE: 0.005702945403754711, Training MAE: 0.005573383532464504\n",
            "Training Iteration 640: Training RMSE: 1.2351279258728027, Training MAE: 1.144191861152649\n",
            "Training Iteration 650: Training RMSE: 0.11846958100795746, Training MAE: 0.11137820780277252\n",
            "Training Iteration 660: Training RMSE: 0.18072667717933655, Training MAE: 0.12997020781040192\n",
            "Training Iteration 670: Training RMSE: 0.27039188146591187, Training MAE: 0.25015759468078613\n",
            "Training Iteration 680: Training RMSE: 0.25464099645614624, Training MAE: 0.2055104672908783\n",
            "Training Iteration 690: Training RMSE: 0.2058555781841278, Training MAE: 0.15564770996570587\n",
            "Training Iteration 700: Training RMSE: 1.3169556856155396, Training MAE: 1.2897157669067383\n",
            "Training Iteration 710: Training RMSE: 0.31572335958480835, Training MAE: 0.2916795611381531\n",
            "Training Iteration 720: Training RMSE: 0.21375389397144318, Training MAE: 0.17903725802898407\n",
            "Training Iteration 730: Training RMSE: 0.17884735763072968, Training MAE: 0.15007765591144562\n",
            "Training Iteration 740: Training RMSE: 0.20768612623214722, Training MAE: 0.1661839485168457\n",
            "Training Iteration 750: Training RMSE: 0.38705310225486755, Training MAE: 0.2791803777217865\n",
            "Training Iteration 760: Training RMSE: 0.20378395915031433, Training MAE: 0.12795120477676392\n",
            "Training Iteration 770: Training RMSE: 0.15844446420669556, Training MAE: 0.1493598073720932\n",
            "Training Iteration 780: Training RMSE: 0.1284019947052002, Training MAE: 0.12212643027305603\n",
            "Training Iteration 790: Training RMSE: 0.20084787905216217, Training MAE: 0.1640738546848297\n",
            "Training Iteration 800: Training RMSE: 0.15375296771526337, Training MAE: 0.13193297386169434\n",
            "Training Iteration 810: Training RMSE: 0.37555673718452454, Training MAE: 0.3263905644416809\n",
            "Training Iteration 820: Training RMSE: 0.18271614611148834, Training MAE: 0.14695478975772858\n",
            "Training Iteration 830: Training RMSE: 0.3774256408214569, Training MAE: 0.2798841595649719\n",
            "Training Iteration 840: Training RMSE: 0.0029524306301027536, Training MAE: 0.002583802444860339\n",
            "Training Iteration 850: Training RMSE: 0.5106610655784607, Training MAE: 0.4533937871456146\n",
            "Training Iteration 860: Training RMSE: 0.9366206526756287, Training MAE: 0.7626723051071167\n",
            "Training Iteration 870: Training RMSE: 0.042948149144649506, Training MAE: 0.035046227276325226\n",
            "Training Iteration 880: Training RMSE: 0.4529622495174408, Training MAE: 0.38538259267807007\n",
            "Training Iteration 890: Training RMSE: 0.511464536190033, Training MAE: 0.3358919620513916\n",
            "Training Iteration 900: Training RMSE: 0.28185775876045227, Training MAE: 0.24734501540660858\n",
            "Training Iteration 910: Training RMSE: 0.21583838760852814, Training MAE: 0.15879669785499573\n",
            "Training Iteration 920: Training RMSE: 0.4036197066307068, Training MAE: 0.2638384699821472\n",
            "Training Iteration 930: Training RMSE: 0.1752915233373642, Training MAE: 0.17179863154888153\n",
            "Training Iteration 940: Training RMSE: 0.1442112922668457, Training MAE: 0.14085866510868073\n",
            "Training Iteration 950: Training RMSE: 0.5906735062599182, Training MAE: 0.5135031938552856\n",
            "Training Iteration 960: Training RMSE: 0.2885291278362274, Training MAE: 0.2488284558057785\n",
            "Training Iteration 970: Training RMSE: 0.1667429655790329, Training MAE: 0.16295550763607025\n",
            "Training Iteration 980: Training RMSE: 0.2156953066587448, Training MAE: 0.16326282918453217\n",
            "Training Iteration 990: Training RMSE: 1.1655240058898926, Training MAE: 0.9694607257843018\n",
            "Training Iteration 1000: Training RMSE: 0.27696195244789124, Training MAE: 0.2594379186630249\n",
            "Training Iteration 1010: Training RMSE: 0.6206936240196228, Training MAE: 0.4341394603252411\n",
            "Training Iteration 1020: Training RMSE: 0.15291643142700195, Training MAE: 0.13917294144630432\n",
            "Training Iteration 1030: Training RMSE: 0.34977585077285767, Training MAE: 0.2764781713485718\n",
            "Training Iteration 1040: Training RMSE: 0.05091029778122902, Training MAE: 0.043331488966941833\n",
            "Training Iteration 1050: Training RMSE: 0.7623154520988464, Training MAE: 0.6112524271011353\n",
            "Training Iteration 1060: Training RMSE: 0.1766505092382431, Training MAE: 0.14884142577648163\n",
            "Training Iteration 1070: Training RMSE: 0.36548107862472534, Training MAE: 0.29731234908103943\n",
            "Training Iteration 1080: Training RMSE: 0.2941741645336151, Training MAE: 0.28967946767807007\n",
            "Training Iteration 1090: Training RMSE: 0.3773133158683777, Training MAE: 0.23918110132217407\n",
            "Training Iteration 1100: Training RMSE: 0.18179243803024292, Training MAE: 0.14412295818328857\n",
            "Training Iteration 1110: Training RMSE: 1.0833805799484253, Training MAE: 0.9827791452407837\n",
            "Training Iteration 1120: Training RMSE: 1.4958611726760864, Training MAE: 1.1601054668426514\n",
            "Training Iteration 1130: Training RMSE: 0.13519272208213806, Training MAE: 0.09874497354030609\n",
            "Training Iteration 1140: Training RMSE: 0.2867361307144165, Training MAE: 0.1822146475315094\n",
            "Training Iteration 1150: Training RMSE: 0.34879907965660095, Training MAE: 0.2624001204967499\n",
            "Training Iteration 1160: Training RMSE: 0.09456530958414078, Training MAE: 0.07772237062454224\n",
            "Training Iteration 1170: Training RMSE: 0.12709026038646698, Training MAE: 0.1252819448709488\n",
            "Training Iteration 1180: Training RMSE: 0.3642641007900238, Training MAE: 0.2610231637954712\n",
            "Training Iteration 1190: Training RMSE: 0.355048805475235, Training MAE: 0.2870352566242218\n",
            "Training Iteration 1200: Training RMSE: 0.21901977062225342, Training MAE: 0.18302986025810242\n",
            "Training Iteration 1210: Training RMSE: 0.2520834505558014, Training MAE: 0.20002809166908264\n",
            "Training Iteration 1220: Training RMSE: 0.30299660563468933, Training MAE: 0.24362432956695557\n",
            "Training Iteration 1230: Training RMSE: 0.324616014957428, Training MAE: 0.23847901821136475\n",
            "Training Iteration 1240: Training RMSE: 0.9357056021690369, Training MAE: 0.6932830214500427\n",
            "Training Iteration 1250: Training RMSE: 0.14393112063407898, Training MAE: 0.12843459844589233\n",
            "Training Iteration 1260: Training RMSE: 0.21270745992660522, Training MAE: 0.16596576571464539\n",
            "Training Iteration 1270: Training RMSE: 0.3950549066066742, Training MAE: 0.3451867699623108\n",
            "Training Iteration 1280: Training RMSE: 0.29726988077163696, Training MAE: 0.2946229577064514\n",
            "Training Iteration 1290: Training RMSE: 1.2196636199951172, Training MAE: 1.1182050704956055\n",
            "Training Iteration 1300: Training RMSE: 1.674346685409546, Training MAE: 1.4847091436386108\n",
            "Training Iteration 1310: Training RMSE: 0.49571532011032104, Training MAE: 0.39905959367752075\n",
            "Training Iteration 1320: Training RMSE: 0.2813139259815216, Training MAE: 0.21187293529510498\n",
            "Training Iteration 1330: Training RMSE: 0.22921249270439148, Training MAE: 0.1788673847913742\n",
            "Training Iteration 1340: Training RMSE: 1.424233078956604, Training MAE: 1.1584895849227905\n",
            "Training Iteration 1350: Training RMSE: 0.35470548272132874, Training MAE: 0.34011420607566833\n",
            "Training Iteration 1360: Training RMSE: 0.06914072483778, Training MAE: 0.0583454929292202\n",
            "Training Iteration 1370: Training RMSE: 0.24481673538684845, Training MAE: 0.23957495391368866\n",
            "Training Iteration 1380: Training RMSE: 0.10226503759622574, Training MAE: 0.07113584876060486\n",
            "Training Iteration 1390: Training RMSE: 2.2724268436431885, Training MAE: 2.01125431060791\n",
            "Training Iteration 1400: Training RMSE: 0.2331007421016693, Training MAE: 0.20856627821922302\n",
            "Training Iteration 1410: Training RMSE: 0.1279846876859665, Training MAE: 0.09913679212331772\n",
            "Training Iteration 1420: Training RMSE: 2.054584264755249, Training MAE: 1.726854681968689\n",
            "Training Iteration 1430: Training RMSE: 0.06809970736503601, Training MAE: 0.05566093325614929\n",
            "Training Iteration 1440: Training RMSE: 1.2231032848358154, Training MAE: 1.0820631980895996\n",
            "Training Iteration 1450: Training RMSE: 0.352135568857193, Training MAE: 0.22447338700294495\n",
            "Training Iteration 1460: Training RMSE: 1.3303006887435913, Training MAE: 1.3075227737426758\n",
            "Training Iteration 1470: Training RMSE: 0.11839042603969574, Training MAE: 0.11666148155927658\n",
            "Training Iteration 1480: Training RMSE: 0.17714549601078033, Training MAE: 0.14701105654239655\n",
            "Training Iteration 1490: Training RMSE: 0.37034234404563904, Training MAE: 0.2987704873085022\n",
            "Training Iteration 1500: Training RMSE: 0.43484294414520264, Training MAE: 0.37595415115356445\n",
            "Training Iteration 1510: Training RMSE: 0.17950847744941711, Training MAE: 0.15393508970737457\n",
            "Training Iteration 1520: Training RMSE: 0.10553842782974243, Training MAE: 0.08522959053516388\n",
            "Training Iteration 1530: Training RMSE: 1.2062574625015259, Training MAE: 1.0406402349472046\n",
            "Training Iteration 1540: Training RMSE: 0.7393518090248108, Training MAE: 0.6107116341590881\n",
            "Training Iteration 1550: Training RMSE: 0.5636248588562012, Training MAE: 0.3583524823188782\n",
            "Training Iteration 1560: Training RMSE: 0.13745301961898804, Training MAE: 0.10805603861808777\n",
            "Training Iteration 1570: Training RMSE: 0.24951188266277313, Training MAE: 0.2151515930891037\n",
            "Training Iteration 1580: Training RMSE: 0.022216076031327248, Training MAE: 0.015048655681312084\n",
            "Training Iteration 1590: Training RMSE: 0.3000842332839966, Training MAE: 0.24295958876609802\n",
            "Training Iteration 1600: Training RMSE: 0.5377654433250427, Training MAE: 0.44725823402404785\n",
            "Training Iteration 1610: Training RMSE: 0.4210766851902008, Training MAE: 0.37154674530029297\n",
            "Training Iteration 1620: Training RMSE: 0.3397948741912842, Training MAE: 0.24826917052268982\n",
            "Training Iteration 1630: Training RMSE: 0.5892667174339294, Training MAE: 0.48202061653137207\n",
            "Training Iteration 1640: Training RMSE: 0.24442574381828308, Training MAE: 0.22137513756752014\n",
            "Training Iteration 1650: Training RMSE: 0.1671084612607956, Training MAE: 0.1343884915113449\n",
            "Training Iteration 1660: Training RMSE: 1.576285719871521, Training MAE: 1.5323801040649414\n",
            "Training Iteration 1670: Training RMSE: 0.006428581662476063, Training MAE: 0.0055552697740495205\n",
            "Training Iteration 1680: Training RMSE: 0.2438502162694931, Training MAE: 0.1591687649488449\n",
            "Training Iteration 1690: Training RMSE: 0.33226388692855835, Training MAE: 0.30015701055526733\n",
            "Training Iteration 1700: Training RMSE: 0.24713511765003204, Training MAE: 0.19800332188606262\n",
            "Training Iteration 1710: Training RMSE: 0.9860016107559204, Training MAE: 0.9174372553825378\n",
            "Training Iteration 1720: Training RMSE: 0.1824800819158554, Training MAE: 0.16255126893520355\n",
            "Training Iteration 1730: Training RMSE: 0.6025287508964539, Training MAE: 0.5107596516609192\n",
            "Training Iteration 1740: Training RMSE: 0.17000241577625275, Training MAE: 0.14332789182662964\n",
            "Training Iteration 1750: Training RMSE: 0.14339837431907654, Training MAE: 0.12509174644947052\n",
            "Training Iteration 1760: Training RMSE: 0.17887850105762482, Training MAE: 0.14342111349105835\n",
            "Training Iteration 1770: Training RMSE: 0.2983112633228302, Training MAE: 0.270744264125824\n",
            "Training Iteration 1780: Training RMSE: 1.2778027057647705, Training MAE: 1.0364570617675781\n",
            "Training Iteration 1790: Training RMSE: 0.011353938840329647, Training MAE: 0.011295713484287262\n",
            "Training Iteration 1800: Training RMSE: 0.12137752771377563, Training MAE: 0.09806784242391586\n",
            "Training Iteration 1810: Training RMSE: 0.18163327872753143, Training MAE: 0.1750599592924118\n",
            "Training Iteration 1820: Training RMSE: 0.16413940489292145, Training MAE: 0.11293961852788925\n",
            "Training Iteration 1830: Training RMSE: 0.20507434010505676, Training MAE: 0.13310451805591583\n",
            "Training Iteration 1840: Training RMSE: 1.1936328411102295, Training MAE: 0.9137804508209229\n",
            "Training Iteration 1850: Training RMSE: 0.01094203069806099, Training MAE: 0.007905138656497002\n",
            "Training Iteration 1860: Training RMSE: 0.2951340973377228, Training MAE: 0.19212447106838226\n",
            "Training Iteration 1870: Training RMSE: 0.017680659890174866, Training MAE: 0.01538860984146595\n",
            "Training Iteration 1880: Training RMSE: 0.20590701699256897, Training MAE: 0.15254956483840942\n",
            "Training Iteration 1890: Training RMSE: 0.10221543908119202, Training MAE: 0.08074366301298141\n",
            "Training Iteration 1900: Training RMSE: 1.0546460151672363, Training MAE: 0.8697044253349304\n",
            "Training Iteration 1910: Training RMSE: 0.20826561748981476, Training MAE: 0.1483883261680603\n",
            "Training Iteration 1920: Training RMSE: 0.1095859482884407, Training MAE: 0.10446685552597046\n",
            "Training Iteration 1930: Training RMSE: 0.20634524524211884, Training MAE: 0.16783460974693298\n",
            "Training Iteration 1940: Training RMSE: 0.1952924281358719, Training MAE: 0.17965352535247803\n",
            "Training Iteration 1950: Training RMSE: 0.29200267791748047, Training MAE: 0.205584317445755\n",
            "Training Iteration 1960: Training RMSE: 0.14799737930297852, Training MAE: 0.11170893162488937\n",
            "Training Iteration 1970: Training RMSE: 1.2807961702346802, Training MAE: 1.1624503135681152\n",
            "Training Iteration 1980: Training RMSE: 0.08727800846099854, Training MAE: 0.07540623843669891\n",
            "Training Iteration 1990: Training RMSE: 0.05890961363911629, Training MAE: 0.05529509112238884\n",
            "Training Iteration 2000: Training RMSE: 1.7228899002075195, Training MAE: 1.4438121318817139\n",
            "Training Iteration 2010: Training RMSE: 0.10971461981534958, Training MAE: 0.09960656613111496\n",
            "Training Iteration 2020: Training RMSE: 0.15468737483024597, Training MAE: 0.12417512387037277\n",
            "Training Iteration 2030: Training RMSE: 0.05857826769351959, Training MAE: 0.05594171583652496\n",
            "Training Iteration 2040: Training RMSE: 1.3190371990203857, Training MAE: 1.1592851877212524\n",
            "Training Iteration 2050: Training RMSE: 0.12502309679985046, Training MAE: 0.1004885584115982\n",
            "Training Iteration 2060: Training RMSE: 1.1416889429092407, Training MAE: 0.891162097454071\n",
            "Training Iteration 2070: Training RMSE: 0.20540599524974823, Training MAE: 0.18676254153251648\n",
            "Training Iteration 2080: Training RMSE: 0.0182691290974617, Training MAE: 0.01599893532693386\n",
            "Training Iteration 2090: Training RMSE: 0.47048884630203247, Training MAE: 0.40914344787597656\n",
            "Training Iteration 2100: Training RMSE: 0.00439048558473587, Training MAE: 0.004123966675251722\n",
            "Training Iteration 2110: Training RMSE: 1.6697852611541748, Training MAE: 1.583227515220642\n",
            "Training Iteration 2120: Training RMSE: 0.2218693494796753, Training MAE: 0.1860828399658203\n",
            "Training Iteration 2130: Training RMSE: 1.666731595993042, Training MAE: 1.5646846294403076\n",
            "Training Iteration 2140: Training RMSE: 0.14570961892604828, Training MAE: 0.11006034910678864\n",
            "Training Iteration 2150: Training RMSE: 0.14920802414417267, Training MAE: 0.08873870968818665\n",
            "Training Iteration 2160: Training RMSE: 0.11691102385520935, Training MAE: 0.10556930303573608\n",
            "Training Iteration 2170: Training RMSE: 0.029779944568872452, Training MAE: 0.026547014713287354\n",
            "Training Iteration 2180: Training RMSE: 0.14224350452423096, Training MAE: 0.11345186084508896\n",
            "Training Iteration 2190: Training RMSE: 0.0070947762578725815, Training MAE: 0.0056016892194747925\n",
            "Training Iteration 2200: Training RMSE: 0.11116157472133636, Training MAE: 0.09672968089580536\n",
            "Training Iteration 2210: Training RMSE: 0.2199350893497467, Training MAE: 0.1716454029083252\n",
            "Training Iteration 2220: Training RMSE: 0.1492624282836914, Training MAE: 0.1330239474773407\n",
            "Training Iteration 2230: Training RMSE: 0.2638619840145111, Training MAE: 0.22275981307029724\n",
            "Training Iteration 2240: Training RMSE: 0.23945561051368713, Training MAE: 0.2146371304988861\n",
            "Training Iteration 2250: Training RMSE: 0.14148029685020447, Training MAE: 0.09357782453298569\n",
            "Training Iteration 2260: Training RMSE: 0.301376074552536, Training MAE: 0.18897917866706848\n",
            "Training Iteration 2270: Training RMSE: 0.22336599230766296, Training MAE: 0.18627123534679413\n",
            "Training Iteration 2280: Training RMSE: 0.22253544628620148, Training MAE: 0.2187134474515915\n",
            "Training Iteration 2290: Training RMSE: 0.6666399836540222, Training MAE: 0.6554058790206909\n",
            "Training Iteration 2300: Training RMSE: 0.15972918272018433, Training MAE: 0.11099544167518616\n",
            "Training Iteration 2310: Training RMSE: 0.11686301976442337, Training MAE: 0.11350463330745697\n",
            "Training Iteration 2320: Training RMSE: 2.479473829269409, Training MAE: 2.4035675525665283\n",
            "Training Iteration 2330: Training RMSE: 0.35738611221313477, Training MAE: 0.264901340007782\n",
            "Training Iteration 2340: Training RMSE: 0.2599259912967682, Training MAE: 0.24346661567687988\n",
            "Training Iteration 2350: Training RMSE: 0.06728597730398178, Training MAE: 0.062044333666563034\n",
            "Training Iteration 2360: Training RMSE: 1.9680042266845703, Training MAE: 1.7981858253479004\n",
            "Training Iteration 2370: Training RMSE: 0.4057423770427704, Training MAE: 0.3540603518486023\n",
            "Training Iteration 2380: Training RMSE: 0.1059141457080841, Training MAE: 0.09839797019958496\n",
            "Training Iteration 2390: Training RMSE: 0.006462267599999905, Training MAE: 0.004883139859884977\n",
            "Training Iteration 2400: Training RMSE: 0.3003164827823639, Training MAE: 0.2586438059806824\n",
            "Training Iteration 2410: Training RMSE: 0.3824097514152527, Training MAE: 0.24716225266456604\n",
            "Training Iteration 2420: Training RMSE: 0.8001495003700256, Training MAE: 0.6595379710197449\n",
            "Training Iteration 2430: Training RMSE: 2.1176884174346924, Training MAE: 1.7294175624847412\n",
            "Training Iteration 2440: Training RMSE: 0.3610624372959137, Training MAE: 0.2957441210746765\n",
            "Training Iteration 2450: Training RMSE: 0.13237981498241425, Training MAE: 0.11334525048732758\n",
            "Training Iteration 2460: Training RMSE: 0.20552097260951996, Training MAE: 0.14451953768730164\n",
            "Training Iteration 2470: Training RMSE: 0.0146391112357378, Training MAE: 0.009930519387125969\n",
            "Training Iteration 2480: Training RMSE: 0.7351269721984863, Training MAE: 0.6973600387573242\n",
            "Training Iteration 2490: Training RMSE: 0.047517526894807816, Training MAE: 0.04743172973394394\n",
            "Training Iteration 2500: Training RMSE: 0.2017754465341568, Training MAE: 0.14217792451381683\n",
            "Training Iteration 2510: Training RMSE: 0.4284636676311493, Training MAE: 0.34687694907188416\n",
            "Training Iteration 2520: Training RMSE: 0.029781399294734, Training MAE: 0.022532552480697632\n",
            "Training Iteration 2530: Training RMSE: 0.12532463669776917, Training MAE: 0.10292758792638779\n",
            "Training Iteration 2540: Training RMSE: 0.15790316462516785, Training MAE: 0.1491573303937912\n",
            "Training Iteration 2550: Training RMSE: 0.9141258001327515, Training MAE: 0.6170001029968262\n",
            "Training Iteration 2560: Training RMSE: 0.11151500046253204, Training MAE: 0.08865711092948914\n",
            "Training Iteration 2570: Training RMSE: 0.0462985523045063, Training MAE: 0.03547277674078941\n",
            "Training Iteration 2580: Training RMSE: 0.13784687221050262, Training MAE: 0.11647365987300873\n",
            "Training Iteration 2590: Training RMSE: 1.4733030796051025, Training MAE: 1.148375391960144\n",
            "Training Iteration 2600: Training RMSE: 1.560664176940918, Training MAE: 1.528476595878601\n",
            "Training Iteration 2610: Training RMSE: 0.35776078701019287, Training MAE: 0.24162951111793518\n",
            "Training Iteration 2620: Training RMSE: 0.012428497895598412, Training MAE: 0.012080918066203594\n",
            "Training Iteration 2630: Training RMSE: 0.9957194924354553, Training MAE: 0.977317214012146\n",
            "Training Iteration 2640: Training RMSE: 0.2153424322605133, Training MAE: 0.15449205040931702\n",
            "Training Iteration 2650: Training RMSE: 0.3133760690689087, Training MAE: 0.26860538125038147\n",
            "Training Iteration 2660: Training RMSE: 0.09010932594537735, Training MAE: 0.07690531015396118\n",
            "Training Iteration 2670: Training RMSE: 1.0600221157073975, Training MAE: 0.8601900935173035\n",
            "Training Iteration 2680: Training RMSE: 0.06042443960905075, Training MAE: 0.05884498357772827\n",
            "Training Iteration 2690: Training RMSE: 0.3249795436859131, Training MAE: 0.2388976663351059\n",
            "Training Iteration 2700: Training RMSE: 1.447091817855835, Training MAE: 1.293688416481018\n",
            "Training Iteration 2710: Training RMSE: 0.1589352786540985, Training MAE: 0.13042142987251282\n",
            "Training Iteration 2720: Training RMSE: 0.2598975598812103, Training MAE: 0.2306187003850937\n",
            "Training Iteration 2730: Training RMSE: 0.010024609975516796, Training MAE: 0.009564347565174103\n",
            "Training Iteration 2740: Training RMSE: 0.5473069548606873, Training MAE: 0.4576553702354431\n",
            "Training Iteration 2750: Training RMSE: 0.005042793694883585, Training MAE: 0.004310445860028267\n",
            "Training Iteration 2760: Training RMSE: 1.4272863864898682, Training MAE: 1.3364980220794678\n",
            "Training Iteration 2770: Training RMSE: 0.35503727197647095, Training MAE: 0.27544915676116943\n",
            "Training Iteration 2780: Training RMSE: 1.3747648000717163, Training MAE: 1.287543773651123\n",
            "Training Iteration 2790: Training RMSE: 0.15870031714439392, Training MAE: 0.1510661095380783\n",
            "Training Iteration 2800: Training RMSE: 0.10890857130289078, Training MAE: 0.10070906579494476\n",
            "Training Iteration 2810: Training RMSE: 0.436756432056427, Training MAE: 0.4220506548881531\n",
            "Training Iteration 2820: Training RMSE: 1.4225759506225586, Training MAE: 1.096134901046753\n",
            "Training Iteration 2830: Training RMSE: 0.0011078125098720193, Training MAE: 0.0008391456212848425\n",
            "Training Iteration 2840: Training RMSE: 1.4009329080581665, Training MAE: 1.3519738912582397\n",
            "Training Iteration 2850: Training RMSE: 0.026165194809436798, Training MAE: 0.021186619997024536\n",
            "Training Iteration 2860: Training RMSE: 0.2629181444644928, Training MAE: 0.21155546605587006\n",
            "Training Iteration 2870: Training RMSE: 0.022681010887026787, Training MAE: 0.016506876796483994\n",
            "Training Iteration 2880: Training RMSE: 0.319400429725647, Training MAE: 0.2779086232185364\n",
            "Training Iteration 2890: Training RMSE: 0.11713889986276627, Training MAE: 0.11644735932350159\n",
            "Training Iteration 2900: Training RMSE: 0.2927496135234833, Training MAE: 0.28461897373199463\n",
            "Training Iteration 2910: Training RMSE: 0.4165719449520111, Training MAE: 0.3611621856689453\n",
            "Training Iteration 2920: Training RMSE: 0.0240988377481699, Training MAE: 0.019872348755598068\n",
            "Training Iteration 2930: Training RMSE: 0.4772016704082489, Training MAE: 0.40528950095176697\n",
            "Training Iteration 2940: Training RMSE: 0.2668933570384979, Training MAE: 0.19382594525814056\n",
            "Training Iteration 2950: Training RMSE: 0.006015509832650423, Training MAE: 0.004911647643893957\n",
            "Training Iteration 2960: Training RMSE: 0.13637177646160126, Training MAE: 0.10667900741100311\n",
            "Training Iteration 2970: Training RMSE: 0.3775022327899933, Training MAE: 0.27801236510276794\n",
            "Training Iteration 2980: Training RMSE: 0.048546355217695236, Training MAE: 0.041209571063518524\n",
            "Training Iteration 2990: Training RMSE: 0.09031351655721664, Training MAE: 0.08676719665527344\n",
            "Training Iteration 3000: Training RMSE: 0.021733656525611877, Training MAE: 0.021168403327465057\n",
            "Training Iteration 3010: Training RMSE: 0.04351236671209335, Training MAE: 0.0347834937274456\n",
            "Training Iteration 3020: Training RMSE: 0.21333090960979462, Training MAE: 0.12702447175979614\n",
            "Training Iteration 3030: Training RMSE: 0.4711514115333557, Training MAE: 0.4226333498954773\n",
            "Training Iteration 3040: Training RMSE: 0.07740333676338196, Training MAE: 0.07163193821907043\n",
            "Training Iteration 3050: Training RMSE: 0.1886523962020874, Training MAE: 0.170485720038414\n",
            "Training Iteration 3060: Training RMSE: 0.011717002838850021, Training MAE: 0.010637326166033745\n",
            "Training Iteration 3070: Training RMSE: 0.30344802141189575, Training MAE: 0.24009883403778076\n",
            "Training Iteration 3080: Training RMSE: 0.35891395807266235, Training MAE: 0.2846675515174866\n",
            "Training Iteration 3090: Training RMSE: 0.20790483057498932, Training MAE: 0.20049206912517548\n",
            "Training Iteration 3100: Training RMSE: 1.167921781539917, Training MAE: 0.9717580676078796\n",
            "Training Iteration 3110: Training RMSE: 0.1572919338941574, Training MAE: 0.12733463943004608\n",
            "Training Iteration 3120: Training RMSE: 0.2738271951675415, Training MAE: 0.22482451796531677\n",
            "Training Iteration 3130: Training RMSE: 2.1389968395233154, Training MAE: 1.8055100440979004\n",
            "Training Iteration 3140: Training RMSE: 0.5546208024024963, Training MAE: 0.4344775080680847\n",
            "Training Iteration 3150: Training RMSE: 0.03515239432454109, Training MAE: 0.03342686966061592\n",
            "Training Iteration 3160: Training RMSE: 0.6025099754333496, Training MAE: 0.4723297953605652\n",
            "Training Iteration 3170: Training RMSE: 0.12928786873817444, Training MAE: 0.10993461310863495\n",
            "Training Iteration 3180: Training RMSE: 0.4951675832271576, Training MAE: 0.41370689868927\n",
            "Training Iteration 3190: Training RMSE: 2.0655906200408936, Training MAE: 1.8759009838104248\n",
            "Training Iteration 3200: Training RMSE: 0.1558016538619995, Training MAE: 0.15243951976299286\n",
            "Training Iteration 3210: Training RMSE: 0.49527016282081604, Training MAE: 0.35573333501815796\n",
            "Training Iteration 3220: Training RMSE: 0.04408993944525719, Training MAE: 0.03715907782316208\n",
            "Training Iteration 3230: Training RMSE: 0.12741103768348694, Training MAE: 0.08582760393619537\n",
            "Training Iteration 3240: Training RMSE: 0.006268528290092945, Training MAE: 0.004325959365814924\n",
            "Training Iteration 3250: Training RMSE: 0.1159220039844513, Training MAE: 0.09809061884880066\n",
            "Training Iteration 3260: Training RMSE: 0.2964473366737366, Training MAE: 0.2224433869123459\n",
            "Training Iteration 3270: Training RMSE: 0.0439409501850605, Training MAE: 0.03902784362435341\n",
            "Training Iteration 3280: Training RMSE: 0.3558451533317566, Training MAE: 0.2928161919116974\n",
            "Training Iteration 3290: Training RMSE: 1.1047215461730957, Training MAE: 1.017265796661377\n",
            "Training Iteration 3300: Training RMSE: 0.5206658244132996, Training MAE: 0.4388185143470764\n",
            "Training Iteration 3310: Training RMSE: 0.4697929620742798, Training MAE: 0.3156352639198303\n",
            "Training Iteration 3320: Training RMSE: 0.8638889193534851, Training MAE: 0.8076023459434509\n",
            "Training Iteration 3330: Training RMSE: 0.011964905075728893, Training MAE: 0.010570804588496685\n",
            "Training Iteration 3340: Training RMSE: 0.37587636709213257, Training MAE: 0.3417619466781616\n",
            "Training Iteration 3350: Training RMSE: 0.24841217696666718, Training MAE: 0.16016283631324768\n",
            "Training Iteration 3360: Training RMSE: 1.0583701133728027, Training MAE: 0.7772785425186157\n",
            "Training Iteration 3370: Training RMSE: 0.09137658774852753, Training MAE: 0.0813756212592125\n",
            "Training Iteration 3380: Training RMSE: 1.2419886589050293, Training MAE: 1.1579985618591309\n",
            "Training Iteration 3390: Training RMSE: 0.14949728548526764, Training MAE: 0.11685812473297119\n",
            "Training Iteration 3400: Training RMSE: 0.39687126874923706, Training MAE: 0.3034742474555969\n",
            "Training Iteration 3410: Training RMSE: 0.28407368063926697, Training MAE: 0.23319515585899353\n",
            "Training Iteration 3420: Training RMSE: 0.12050527334213257, Training MAE: 0.10007400810718536\n",
            "Training Iteration 3430: Training RMSE: 0.004092019516974688, Training MAE: 0.0037930714897811413\n",
            "Training Iteration 3440: Training RMSE: 0.0025811668019741774, Training MAE: 0.002265715040266514\n",
            "Training Iteration 3450: Training RMSE: 0.009391979314386845, Training MAE: 0.008569089695811272\n",
            "Training Iteration 3460: Training RMSE: 0.028401436284184456, Training MAE: 0.023826364427804947\n",
            "Training Iteration 3470: Training RMSE: 0.3222160339355469, Training MAE: 0.25233137607574463\n",
            "Training Iteration 3480: Training RMSE: 0.17395153641700745, Training MAE: 0.14133931696414948\n",
            "Training Iteration 3490: Training RMSE: 0.09536176174879074, Training MAE: 0.09496112167835236\n",
            "Training Iteration 3500: Training RMSE: 0.2190694808959961, Training MAE: 0.1691182255744934\n",
            "Training Iteration 3510: Training RMSE: 1.2929847240447998, Training MAE: 1.0706911087036133\n",
            "Training Iteration 3520: Training RMSE: 0.08091775327920914, Training MAE: 0.07761048525571823\n",
            "Training Iteration 3530: Training RMSE: 0.26693782210350037, Training MAE: 0.19925129413604736\n",
            "Training Iteration 3540: Training RMSE: 1.7357094287872314, Training MAE: 1.5434832572937012\n",
            "Training Iteration 3550: Training RMSE: 0.14001192152500153, Training MAE: 0.126089408993721\n",
            "Training Iteration 3560: Training RMSE: 1.6537795066833496, Training MAE: 1.4526857137680054\n",
            "Training Iteration 3570: Training RMSE: 0.1782122403383255, Training MAE: 0.1516733318567276\n",
            "Training Iteration 3580: Training RMSE: 0.9475107192993164, Training MAE: 0.8436592817306519\n",
            "Training Iteration 3590: Training RMSE: 0.2633425295352936, Training MAE: 0.24583035707473755\n",
            "Training Iteration 3600: Training RMSE: 1.0301978588104248, Training MAE: 0.8071136474609375\n",
            "Training Iteration 3610: Training RMSE: 0.0784377008676529, Training MAE: 0.07068463414907455\n",
            "Training Iteration 3620: Training RMSE: 0.29094889760017395, Training MAE: 0.23397833108901978\n",
            "Training Iteration 3630: Training RMSE: 0.5040108561515808, Training MAE: 0.3101959228515625\n",
            "Training Iteration 3640: Training RMSE: 0.22514724731445312, Training MAE: 0.18855546414852142\n",
            "Training Iteration 3650: Training RMSE: 0.06467258930206299, Training MAE: 0.058392420411109924\n",
            "Training Iteration 3660: Training RMSE: 0.05787646397948265, Training MAE: 0.04651176929473877\n",
            "Training Iteration 3670: Training RMSE: 0.21107448637485504, Training MAE: 0.14896252751350403\n",
            "Training Iteration 3680: Training RMSE: 0.081004798412323, Training MAE: 0.06391355395317078\n",
            "Training Iteration 3690: Training RMSE: 0.24633583426475525, Training MAE: 0.1935453861951828\n",
            "Training Iteration 3700: Training RMSE: 0.003350932849571109, Training MAE: 0.0028889074455946684\n",
            "Training Iteration 3710: Training RMSE: 0.014938590116798878, Training MAE: 0.011041570454835892\n",
            "Training Iteration 3720: Training RMSE: 0.14534658193588257, Training MAE: 0.13493871688842773\n",
            "Training Iteration 3730: Training RMSE: 0.2660643756389618, Training MAE: 0.2589159607887268\n",
            "Training Iteration 3740: Training RMSE: 0.4602918028831482, Training MAE: 0.4213930070400238\n",
            "Training Iteration 3750: Training RMSE: 0.4326702654361725, Training MAE: 0.3497866094112396\n",
            "Training Iteration 3760: Training RMSE: 1.3372414112091064, Training MAE: 1.0405964851379395\n",
            "Training Iteration 3770: Training RMSE: 0.20232827961444855, Training MAE: 0.16952034831047058\n",
            "Training Iteration 3780: Training RMSE: 0.19035029411315918, Training MAE: 0.12722435593605042\n",
            "Training Iteration 3790: Training RMSE: 0.13493826985359192, Training MAE: 0.11590784043073654\n",
            "Training Iteration 3800: Training RMSE: 0.36147820949554443, Training MAE: 0.24581670761108398\n",
            "Training Iteration 3810: Training RMSE: 0.07723050564527512, Training MAE: 0.054284971207380295\n",
            "Training Iteration 3820: Training RMSE: 0.1701488345861435, Training MAE: 0.14181850850582123\n",
            "Training Iteration 3830: Training RMSE: 0.15806415677070618, Training MAE: 0.14515185356140137\n",
            "Training Iteration 3840: Training RMSE: 2.3388144969940186, Training MAE: 1.9674967527389526\n",
            "Training Iteration 3850: Training RMSE: 0.1364547461271286, Training MAE: 0.10094364732503891\n",
            "Training Iteration 3860: Training RMSE: 0.019961608573794365, Training MAE: 0.01746164634823799\n",
            "Training Iteration 3870: Training RMSE: 2.0218405723571777, Training MAE: 1.7380399703979492\n",
            "Training Iteration 3880: Training RMSE: 0.2668096125125885, Training MAE: 0.22039613127708435\n",
            "Training Iteration 3890: Training RMSE: 0.5212971568107605, Training MAE: 0.49842092394828796\n",
            "Training Iteration 3900: Training RMSE: 0.12354033440351486, Training MAE: 0.08300137519836426\n",
            "Training Iteration 3910: Training RMSE: 0.1637309044599533, Training MAE: 0.12817101180553436\n",
            "Training Iteration 3920: Training RMSE: 0.040397100150585175, Training MAE: 0.03592776879668236\n",
            "Training Iteration 3930: Training RMSE: 0.04579884931445122, Training MAE: 0.033684369176626205\n",
            "Training Iteration 3940: Training RMSE: 0.2897213101387024, Training MAE: 0.19655254483222961\n",
            "Training Iteration 3950: Training RMSE: 0.19604164361953735, Training MAE: 0.16574111580848694\n",
            "Training Iteration 3960: Training RMSE: 0.07045482844114304, Training MAE: 0.04758588969707489\n",
            "Training Iteration 3970: Training RMSE: 0.029565000906586647, Training MAE: 0.02710343897342682\n",
            "Training Iteration 3980: Training RMSE: 0.16715316474437714, Training MAE: 0.160259410738945\n",
            "Training Iteration 3990: Training RMSE: 0.189249187707901, Training MAE: 0.15757395327091217\n",
            "Training Iteration 4000: Training RMSE: 0.26601171493530273, Training MAE: 0.232915997505188\n",
            "Training Iteration 4010: Training RMSE: 0.0040091401897370815, Training MAE: 0.0030315746553242207\n",
            "Training Iteration 4020: Training RMSE: 0.5727479457855225, Training MAE: 0.42687082290649414\n",
            "Training Iteration 4030: Training RMSE: 0.10327736288309097, Training MAE: 0.07785052061080933\n",
            "Training Iteration 4040: Training RMSE: 0.5127726197242737, Training MAE: 0.3750377595424652\n",
            "Training Iteration 4050: Training RMSE: 0.13477060198783875, Training MAE: 0.12263612449169159\n",
            "Training Iteration 4060: Training RMSE: 0.09725051373243332, Training MAE: 0.08093252032995224\n",
            "Training Iteration 4070: Training RMSE: 0.1804230660200119, Training MAE: 0.14315515756607056\n",
            "Training Iteration 4080: Training RMSE: 0.20478679239749908, Training MAE: 0.15585768222808838\n",
            "Training Iteration 4090: Training RMSE: 0.31432172656059265, Training MAE: 0.2583220899105072\n",
            "Training Iteration 4100: Training RMSE: 0.7116384506225586, Training MAE: 0.6983635425567627\n",
            "Training Iteration 4110: Training RMSE: 0.41538622975349426, Training MAE: 0.3417983055114746\n",
            "Training Iteration 4120: Training RMSE: 0.32645776867866516, Training MAE: 0.2639431357383728\n",
            "Training Iteration 4130: Training RMSE: 0.13999998569488525, Training MAE: 0.0981045588850975\n",
            "Training Iteration 4140: Training RMSE: 0.29667147994041443, Training MAE: 0.2038985788822174\n",
            "Training Iteration 4150: Training RMSE: 0.01599453017115593, Training MAE: 0.012559938244521618\n",
            "Training Iteration 4160: Training RMSE: 1.2523510456085205, Training MAE: 0.8637072443962097\n",
            "Training Iteration 4170: Training RMSE: 0.21701645851135254, Training MAE: 0.17289787530899048\n",
            "Training Iteration 4180: Training RMSE: 0.4095991253852844, Training MAE: 0.33816179633140564\n",
            "Training Iteration 4190: Training RMSE: 0.263644278049469, Training MAE: 0.23911404609680176\n",
            "Training Iteration 4200: Training RMSE: 0.2442372888326645, Training MAE: 0.1995048224925995\n",
            "Training Iteration 4210: Training RMSE: 0.15696747601032257, Training MAE: 0.09624609351158142\n",
            "Training Iteration 4220: Training RMSE: 0.2115117609500885, Training MAE: 0.17673657834529877\n",
            "Training Iteration 4230: Training RMSE: 0.1094164028763771, Training MAE: 0.07268024981021881\n",
            "Training Iteration 4240: Training RMSE: 1.1238826513290405, Training MAE: 1.0197967290878296\n",
            "Training Iteration 4250: Training RMSE: 0.21206387877464294, Training MAE: 0.16915473341941833\n",
            "Training Iteration 4260: Training RMSE: 0.005532678682357073, Training MAE: 0.005236933007836342\n",
            "Training Iteration 4270: Training RMSE: 0.0021849521435797215, Training MAE: 0.001943780342116952\n",
            "Training Iteration 4280: Training RMSE: 0.21532146632671356, Training MAE: 0.15412384271621704\n",
            "Training Iteration 4290: Training RMSE: 0.2468607872724533, Training MAE: 0.17522752285003662\n",
            "Training Iteration 4300: Training RMSE: 0.1323714554309845, Training MAE: 0.13103830814361572\n",
            "Training Iteration 4310: Training RMSE: 0.07356303185224533, Training MAE: 0.06504331529140472\n",
            "Training Iteration 4320: Training RMSE: 0.3135596215724945, Training MAE: 0.2679388225078583\n",
            "Training Iteration 4330: Training RMSE: 0.2991344630718231, Training MAE: 0.228518545627594\n",
            "Training Iteration 4340: Training RMSE: 1.8673062324523926, Training MAE: 1.6052206754684448\n",
            "Training Iteration 4350: Training RMSE: 1.3230994939804077, Training MAE: 1.3142671585083008\n",
            "Training Iteration 4360: Training RMSE: 0.1797669231891632, Training MAE: 0.15169216692447662\n",
            "Training Iteration 4370: Training RMSE: 0.3961437940597534, Training MAE: 0.2684996724128723\n",
            "Training Iteration 4380: Training RMSE: 0.3219360411167145, Training MAE: 0.2661646604537964\n",
            "Training Iteration 4390: Training RMSE: 0.9808787107467651, Training MAE: 0.7248061895370483\n",
            "Training Iteration 4400: Training RMSE: 0.035440463572740555, Training MAE: 0.030239075422286987\n",
            "Training Iteration 4410: Training RMSE: 0.19369447231292725, Training MAE: 0.16878066956996918\n",
            "Training Iteration 4420: Training RMSE: 0.2543560862541199, Training MAE: 0.2397874891757965\n",
            "Training Iteration 4430: Training RMSE: 0.3254014253616333, Training MAE: 0.22572803497314453\n",
            "Training Iteration 4440: Training RMSE: 0.2241954505443573, Training MAE: 0.21381619572639465\n",
            "Training Iteration 4450: Training RMSE: 0.06044841930270195, Training MAE: 0.04756637662649155\n",
            "Training Iteration 4460: Training RMSE: 0.07599654048681259, Training MAE: 0.06608670204877853\n",
            "Training Iteration 4470: Training RMSE: 0.31829920411109924, Training MAE: 0.3066965937614441\n",
            "Training Iteration 4480: Training RMSE: 0.1431068480014801, Training MAE: 0.138014554977417\n",
            "Training Iteration 4490: Training RMSE: 0.4734112620353699, Training MAE: 0.36718320846557617\n",
            "Training Iteration 4500: Training RMSE: 0.3425831198692322, Training MAE: 0.3057901859283447\n",
            "Training Iteration 4510: Training RMSE: 0.4287428855895996, Training MAE: 0.3484148681163788\n",
            "Training Iteration 4520: Training RMSE: 0.36793529987335205, Training MAE: 0.32684755325317383\n",
            "Training Iteration 4530: Training RMSE: 0.38544532656669617, Training MAE: 0.2880289554595947\n",
            "Training Iteration 4540: Training RMSE: 0.6665260195732117, Training MAE: 0.5319245457649231\n",
            "Training Iteration 4550: Training RMSE: 0.37490081787109375, Training MAE: 0.29052257537841797\n",
            "Training Iteration 4560: Training RMSE: 0.8698732852935791, Training MAE: 0.7133201956748962\n",
            "Training Iteration 4570: Training RMSE: 0.1727149486541748, Training MAE: 0.12532326579093933\n",
            "Training Iteration 4580: Training RMSE: 0.355350136756897, Training MAE: 0.34437641501426697\n",
            "Training Iteration 4590: Training RMSE: 0.25477904081344604, Training MAE: 0.18644267320632935\n",
            "Training Iteration 4600: Training RMSE: 0.3071780502796173, Training MAE: 0.20090118050575256\n",
            "Training Iteration 4610: Training RMSE: 0.02317756973206997, Training MAE: 0.01832602545619011\n",
            "Training Iteration 4620: Training RMSE: 0.20785243809223175, Training MAE: 0.17505508661270142\n",
            "Training Iteration 4630: Training RMSE: 0.1459091156721115, Training MAE: 0.11619651317596436\n"
          ]
        }
      ],
      "source": [
        "def training_loop(net, train_loader, val_loader, optimizer, loss_fn):\n",
        "  num_epochs = 1\n",
        "  loss_l1 = nn.L1Loss()\n",
        "\n",
        "  # Store loss history for future plotting\n",
        "  loss_history, test_loss_history  = [], []\n",
        "  loss_history_mae, test_loss_history_mae = [], []\n",
        "  loss, test_loss = 0, 0\n",
        "  history = dict()\n",
        "  history_val = dict()\n",
        "  counter = 0\n",
        "  val_counter = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    batch = iter(train_loader)\n",
        "    for data, targets in batch: # Training loop\n",
        "      optimizer.zero_grad() # Clear gradients for next train\n",
        "\n",
        "      data = data.cuda()\n",
        "      targets = targets.cuda()\n",
        "      net.train() # Forward pass\n",
        "      prediction = net(data) # Predictions\n",
        "\n",
        "      loss = torch.sqrt(loss_fn(prediction, targets))\n",
        "      loss_mae = loss_l1(prediction, targets)\n",
        "      loss_history.append(loss.item())\n",
        "      loss_history_mae.append(loss_mae.item())\n",
        "\n",
        "      # Gradient calculation and weight update\n",
        "      loss.backward() # Backpropagation, compute gradients\n",
        "      optimizer.step() # Performs the update (apply gradients)\n",
        "   \n",
        "      if counter % 10 == 0: # Print every 10 results\n",
        "        print(f\"Training Iteration {counter}: Training RMSE: {loss.item()}, Training MAE: {loss_mae.item()}\")\n",
        "      counter += 1\n",
        "\n",
        "    with torch.no_grad(): # Test loop - do not track history for backpropagation\n",
        "      net.eval() # Test forward pass\n",
        "      test_batch = iter(val_loader)\n",
        "      for test_data, test_targets in test_batch:\n",
        "        test_data = test_data.cuda()\n",
        "        test_targets = test_targets.cuda()\n",
        "        test_pred = net(test_data) # Predictions\n",
        "\n",
        "        test_loss = torch.sqrt(loss_fn(test_pred, test_targets))\n",
        "        test_loss_mae = loss_l1(test_pred, test_targets)\n",
        "        test_loss_history.append(test_loss.item())\n",
        "        test_loss_history_mae.append(test_loss_mae.item())\n",
        "\n",
        "        if val_counter % 10 == 0:\n",
        "          print(f\"Validation Iteration {val_counter}: Validation RMSE: {test_loss.item()}, Validation MAE: {test_loss_mae.item()}\")\n",
        "        val_counter += 1\n",
        "\n",
        "  history['Training RMSE'] = loss_history\n",
        "  history['Training MAE'] = loss_history_mae\n",
        "  history_val['Validation RMSE'] = test_loss_history\n",
        "  history_val['Validation MAE'] = test_loss_history_mae\n",
        "  torch.save(net.state_dict(), SRC + 'results/model2.pt')\n",
        "  return history, history_val\n",
        "\n",
        "history, history_val = training_loop(net, train_loader, val_loader, optimizer, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NG4OrgPBtFb"
      },
      "outputs": [],
      "source": [
        "history_df = pd.DataFrame.from_dict(history)\n",
        "history_val_df = pd.DataFrame.from_dict(history_val)\n",
        "history_df.to_csv(SRC + 'results/history_model2.csv')\n",
        "history_val_df.to_csv(SRC + 'results/history_val_model2.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}